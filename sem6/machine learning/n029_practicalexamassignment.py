# -*- coding: utf-8 -*-
"""N029_PracticalExamAssignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APJI9dovmDwApLZWXp8bxNa_7Xvrigql

Name: **Mohit Parekh**

Class: MBA TECH CE

Sem: VI

Roll No.: **N029**

**Installing Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""**Importing the Dataset**"""

data = pd.read_csv('winequality-red.csv')

#how many rows and columns of data
data.shape

"""Therefore there are:

Rows = 1599

Columns = 12
"""

#datatypes of all attributes in dataset
data.dtypes

#ststistical parameters of data
data.describe()

#Check for the null values
data.isnull().sum()

"""Thus, there is no null value in the dataset"""

#count of different qualities of wine
data['quality'].nunique()

data.quality.unique()

"""There are 6 different qualities of wine. 

Which are 3,4,5,6,7,8

**Data Visualization**

**Bivariate Analysis**
"""

# checking the variation of fixed acidity in the different qualities of wine

plt.scatter(data['quality'], data['fixed acidity'], color = 'green')
plt.title('relation of fixed acidity with wine')
plt.xlabel('quality')
plt.ylabel('fixed acidity')
plt.legend()
plt.show()

# Composition of citric acid go higher as we go higher in the quality of the wine

import seaborn as sns

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'citric acid', data = data)

#Relationship between chloride and quality of wine

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'chlorides', data = data)

#Relationship between free sulphur oxide and quality of wine
fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'free sulfur dioxide', data = data)

#Sulphates and the quality of wine

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'sulphates', data = data)

fig = plt.figure(figsize = (10,6))
sns.heatmap(data.corr(), cmap="coolwarm", annot=True)
#check for correlation between features and target variable

"""From the above correlation plot for the given dataset for wine quality prediction, we can easily see which items are related strongly with each other and which items are related weekly with each other.
For Example, 
# strongly correlated items are :

Fixed acidity is strongly correlated with density

Fixed acidity is strongly correlated with pH 

Fixed acidity is strongly correlated with citric acid

Free sulphur dioxide and total sulphur dioxide have a strong correlation.


# weakly correlated items are :

Density is weakly correalted with volatile acidity and free sulphur dioxide.

"""

#check the pairplot
sns.pairplot(data)

"""**Data pre-processing**"""

# Removing Unnecassary columns from the dataset
# As we saw that volatile acidity, total sulphor dioxide, chlorides, density are very less related to the dependent variable 
#   quality so even if we remove these columns the accuracy won't be affected that much.

data = data.drop(['volatile acidity', 'total sulfur dioxide', 'chlorides', 'density'], axis = 1)

# checking the shape of the dataset
print(data.shape)

data.columns

# converting the response variables(3-7) as binary response variables that is either good or bad

#names = ['bad', 'good']
#bins = (2, 6.5, 8)

#data['quality'] = pd.cut(data['quality'], bins = bins, labels = names)

data['quality'] = data['quality'].map({3 : 'bad', 4 :'bad', 5: 'bad',
                                      6: 'good', 7: 'good', 8: 'good'})

# analyzing the different values present in the dependent variable(quality column)
data['quality'].value_counts()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

data['quality'] = le.fit_transform(data['quality'])

data['quality'].value_counts

sns.countplot(data['quality'])

# dividing the dataset into dependent and independent variables
x = data.drop('quality', axis = 1)
y = data['quality']

# determining the shape of x and y.
print('x shape =',x.shape)
print('y shape =',y.shape)

from sklearn.model_selection import train_test_split
# dividing the dataset in training and testing set with 75-25 ratio
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)

# determining the shapes of training and testing sets
x_train.shape,x_test.shape,y_train.shape,y_test.shape

# standard scaling 

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit_transform(x_train)

scaler = StandardScaler()
scaler.fit_transform(x_test)

"""**Modelling**

**Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.model_selection import GridSearchCV, cross_val_score



# creating the model
model = LogisticRegression()

# feeding the training set into the model
model.fit(x_train, y_train)

# predicting the results for the test set
y_pred = model.predict(x_test)

# calculating the training and testing accuracies
print("Training accuracy :", model.score(x_train,y_train))
print("Testing accuracy :", model.score(x_test,y_test))

# classification report
print(classification_report(y_test,y_pred))

# confusion matrix
print(confusion_matrix(y_test,y_pred))

"""**Support Vector Machine**"""

from sklearn.svm import SVC

# creating the model
model = SVC()

# feeding the training set into the model
model.fit(x_train,y_train)

# predicting the results for the test set
y_pred = model.predict(x_test)

# calculating the training and testing accuracies
print("Training accuracy :", model.score(x_train,y_train))
print("Testing accuracy :", model.score(x_test,y_test))

# finding the best parameters for the SVC model

param = {
    'C': [0.8,0.9,1,1.1,1.2,1.3,1.4],
    'kernel':['linear', 'rbf'],
    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]
}
grid_svc = GridSearchCV(model, param_grid = param, scoring = 'accuracy', cv = 10)

grid_svc.fit(x_train,y_train)

grid_svc.best_params_

# creating a new SVC model with these best parameters

model2 = SVC(C = grid_svc.best_params_['C'] , gamma = grid_svc.best_params_['gamma'], kernel = grid_svc.best_params_['kernel'])
model2.fit(x_train,y_train)
y_pred = model2.predict(x_test)

print(classification_report(y_test, y_pred))

"""**Decision Forest**"""

from sklearn.tree import DecisionTreeClassifier

# creating model
model = DecisionTreeClassifier()

# feeding the training set into the model
model.fit(x_train,y_train)

# predicting the results for the test set
y_pred = model.predict(x_test)

# calculating the training and testing accuracies
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_test,y_test))

# classification report
print(classification_report(y_test,y_pred))

# confusion matrix
print(confusion_matrix(y_test,y_pred))

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

# creating the model
model = RandomForestClassifier(n_estimators = 250)

# feeding the training set into the model
model.fit(x_train,y_train)

# predicting the results for the test set
y_pred = model.predict(x_test)

# calculating the training and testing accuracies
print("Training accuracy :", model.score(x_train, y_train))
print("Testing accuracy :", model.score(x_test, y_test))

# classification report
print(classification_report(y_test,y_pred))

# confusion matrix
print(confusion_matrix(y_test,y_pred))

"""# Conclusion

# **Testing Accuracy:**

Logistic Regression: 71%

SVM: 68.75%

Decision Tree: 72.75%

Random Forest: 77.5%


***Therefore based on the values of accuracies we can say that the best model for this particular dataset is Random forest.***
"""

