# -*- coding: utf-8 -*-
"""WineQuality-Copy1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9_Fnd5rRDb4F_Dp2Xi_ZsMqnqZeoUas

**Installing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score
# %matplotlib inline

"""**Importing the Dataset**"""

data = pd.read_csv('winequality-red.csv')

data.head()

#how many rows and columns of data
data.info()

"""There we can say that there are 12 columns and 1599 rows"""

#datatypes of all attributes in dataset
data.dtypes

#ststistical parameters of data
data.describe()

#Check for the null values
data.isnull()

data.isnull().sum()

"""Therefore there are no null values."""

#count of different qualities of wine
print(data.quality.nunique(dropna=True))
data.quality.unique()

"""**Data Visualization**

**Bivariate Analysis**
"""

# checking the variation of fixed acidity in the different qualities of wine

plt.scatter(data['quality'], data['fixed acidity'], color = 'green')
plt.title('relation of fixed acidity with wine')
plt.xlabel('quality')
plt.ylabel('fixed acidity')
plt.legend()
plt.show()

# Composition of citric acid go higher as we go higher in the quality of the wine

import seaborn as sns

fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'citric acid', data = data)

#Relationship between chloride and quality of wine
fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'chlorides', data = data)

#Relationship between free sulphur oxide and quality of wine
fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'free sulfur dioxide', data = data)

#Sulphates and the quality of wine
fig = plt.figure(figsize = (10,6))
sns.barplot(x = 'quality', y = 'sulphates', data = data)

#check for correlation between features and target variable
corr = data.corr()
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr, cmap='coolwarm', annot=True, fmt=".2f")

"""From the above correlation plot for the given dataset for wine quality prediction, we can easily see which items are related strongly with each other and which items are related weekly with each other.
For Example, 
# The strongly correlated items are :




# The weekly correlated items are :


"""

#check the pairplot
sns.pairplot(data)

"""**Data pre-processing**"""

# Removing Unnecassary columns from the dataset
# As we saw that volatile acidity, total sulphor dioxide, chlorides, density are very less related to the dependent variable 
#   quality so even if we remove these columns the accuracy won't be affected that much.

#data = data.drop(['volatile acidity', 'total sulfur dioxide', 'chlorides', 'density'], axis = 1)

# checking the shape of the dataset
#print(data.shape)

data.columns

# converting the response variables(3-7) as binary response variables that is either good or bad

#names = ['bad', 'good']
#bins = (2, 6.5, 8)

#data['quality'] = pd.cut(data['quality'], bins = bins, labels = names)

data['quality'] = data['quality'].map({3 : 'bad', 4 :'bad', 5: 'bad',
                                      6: 'good', 7: 'good', 8: 'good'})

# analyzing the different values present in the dependent variable(quality column)
data['quality'].value_counts()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

data['quality'] = le.fit_transform(data['quality'])

data['quality'].value_counts

sns.countplot(data['quality'])

# dividing the dataset into dependent and independent variables
X = data.drop('quality', axis = 1)
Y = data['quality']

# determining the shape of x and y.
X.shape, Y.shape

# dividing the dataset in training and testing set with 75-25 ratio
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 42)

# determining the shapes of training and testing sets
X_train.shape , Y_train.shape

X_test.shape, Y_test.shape

# standard scaling 
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""**Modelling**

**Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.model_selection import GridSearchCV, cross_val_score

# creating the model
model = LogisticRegression()

# feeding the training set into the model
model.fit(X_train,Y_train)

# predicting the results for the test set
y_pred = model.predict(X_test)

# calculating the training and testing accuracies
print("Training accuracy :", model.score(X_train,y_train))
print("Testing accuracy :", model.score(X_test,y_test))

# classification report
print(classification_report(y_pred,y_test))

# confusion matrix
print(confusion_matrix(y_pred,y_test))

"""**Support Vector Machine**"""

from sklearn.svm import SVC

# creating the model
model = SVC()

# feeding the training set into the model
model.fit()

# predicting the results for the test set
y_pred = model.predict()

# calculating the training and testing accuracies
print("Training accuracy :", model.score())
print("Testing accuracy :", model.score())

# finding the best parameters for the SVC model

param = {
    'C': [0.8,0.9,1,1.1,1.2,1.3,1.4],
    'kernel':['linear', 'rbf'],
    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]
}
grid_svc = GridSearchCV(model, param_grid = param, scoring = 'accuracy', cv = 10)

grid_svc.fit()

grid_svc.best_params_

# creating a new SVC model with these best parameters

model2 = SVC(C = , gamma = , kernel = '')
model2.fit()
y_pred = model2.predict()

print(classification_report())

"""**Decision Forest**"""

from sklearn.tree import DecisionTreeClassifier

# creating model
model = DecisionTreeClassifier()

# feeding the training set into the model
model.fit()

# predicting the results for the test set
y_pred = model.predict()

# calculating the training and testing accuracies
print("Training accuracy :", model.score())
print("Testing accuracy :", model.score())

# classification report
print(classification_report())

# confusion matrix
print(confusion_matrix())

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

# creating the model
model = RandomForestClassifier(n_estimators =)

# feeding the training set into the model
model.fit()

# predicting the results for the test set
y_pred = model.predict()

# calculating the training and testing accuracies
print("Training accuracy :", model.score())
print("Testing accuracy :", model.score())

# classification report
print(classification_report())

# confusion matrix
print(confusion_matrix())