# -*- coding: utf-8 -*-
"""Implementing Gradient Descent for Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AkgB0Vhv05h5dLECvIELgjsUinA4VW0O

# Loading the dataset
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
sales_df = pd.read_csv( 'Advertising.csv' )
# Pring first few records
sales_df.head()

"""# Set X and Y Variable"""

X = sales_df[['TV','Radio','Newspaper']]
Y = sales_df['Sales']

"""# Standardize X & Y"""

Y = np.array( (Y - Y.mean() ) / Y.std() )
X = X.apply( lambda rec: ( rec - rec.mean() ) / rec.std(),
axis = 0 )

"""# Implementing the Gradient Descent Algorithm
#  Random Initialization the bias and weights
"""

import random
def initialize(dim):  #dim means total weights we want to initialize
  np.random.seed(seed=48)
  random.seed(48)
  b=random.random()    #Initialize the bias
  theta=np.random.rand(dim)      #Initialize the weights.
  return b,theta

b,theta=initialize(3)
print("Bias: ",b,"Weights: ",theta)

"""# Predict Y values from the bias and weights"""

# Inputs:
# b - bias
# w - weights
# X - the input matrix
def predict_Y(b,theta,X):
  return b + np.dot(X,theta)

b,theta=initialize(3)
Y_hat=predict_Y(b,theta,X)

Y_hat[0:10]

Y.shape

"""# Calculate the cost function: MSE"""

import math
# Inputs
# Y - Actual values of y
# Y_hat - predicted value of y
def get_cost(Y,Y_hat):
  Y_resd=Y-Y_hat      # Calculating the residuals from taking difference between actual and predicted values
  return np.sum(np.dot(Y_resd.T,Y_resd))/(2*len(Y-Y_resd))

b,theta=initialize(3)
Y_hat=predict_Y(b,theta,X)
get_cost(Y,Y_hat)

"""# Update the bias and weights"""

def update_theta(x,y,y_hat,b_0,theta_o,learning_rate):
  #gradient of bias
  db=(np.sum(y_hat-y))/len(y)
  #gradient of weights
  dw=(np.sum(np.dot((y_hat-y),x)))/len(y)
    #update bias
  b_1=b_0-learning_rate*db
#update theta
  theta_1=theta_o-learning_rate*dw
    #return the new bias and beta values
  return b_1,theta_1

b,theta=initialize(3)
print("After initialization -Bias: ",b,"theta: ",theta)
Y_hat=predict_Y(b,theta,X)
b,theta=update_theta(X,Y,Y_hat,b,theta,0.001)
print("After first update -Bias: ",b,"theta: ",theta)
#get_cost(Y,Y_hat)

"""# Finding the optimal bias and weights"""

def run_gradient_descent(X,
                         Y,
                         alpha=0.001,
                         num_iterations=500):
# Intialize the bias and weights
  b,theta=initialize(X.shape[1])
  iter_num=0
  # gd_iterations_df keeps track of the cost every 10 iterations
  gd_iterations_df=pd.DataFrame(columns=['iteration','cost'])
  result_idx=0
    
    # Run the iterations in loop

  for each_iter in range(num_iterations):
    Y_hat=predict_Y(b,theta,X)          # Calcuated predicted value of y
    this_cost=get_cost(Y,Y_hat)         # Calculate the cost
    prev_b=b                            # Save the previous bias and weights
    prev_theta=theta
    b,theta=update_theta(X,Y,Y_hat,prev_b,prev_theta,alpha)       # Update and calculate the new values of bias and weights

    if(iter_num%10==0):                                       # For every 10 iterations, store the cost i.e. MSE
      gd_iterations_df.loc[result_idx]=[iter_num,this_cost]
      result_idx=result_idx+1
    iter_num +=1
  print("Final Estimate of b and theta : ",b,theta)
#return the final bias, weights and the cost at the end

  return gd_iterations_df,b,theta

gd_iterations_df,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=500)

gd_iterations_df[0:20]

"""# Plotting the cost function against the iterations"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sn
# %matplotlib inline
plt.plot(gd_iterations_df['iteration'],gd_iterations_df['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

alpha_df_1,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=500)

alpha_df_2,b,theta=run_gradient_descent(X,Y,alpha=0.01,num_iterations=500)

alpha_df_3,b,theta=run_gradient_descent(X,Y,alpha=0.5,num_iterations=500)

#alpha_df_2,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=3000)

plt.plot(alpha_df_1['iteration'],alpha_df_1['cost'],label="alpha=0.001")
plt.plot(alpha_df_2['iteration'],alpha_df_2['cost'],label="alpha=0.01")
plt.plot(alpha_df_3['iteration'],alpha_df_3['cost'],label="alpha=1")
plt.legend()
plt.ylabel('cost')
plt.xlabel('Number of iterations')
plt.title('Cost Vs. Iterations for different alpha values')



