# -*- coding: utf-8 -*-
"""Breast Cancer Classification using Support Vector Machine (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hRJPvXUfiu8fcPzfsXCNJ9HIax7XAcaM

In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. 
Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier . An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. This gap is also called maximum margin and the SVM classifier is called maximum margin clasifier.

 In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.datasets import load_breast_cancer

cancer=load_breast_cancer()

cancer.keys()

print(cancer['DESCR'])

cancer['feature_names']

df=pd.DataFrame(cancer['data'],columns=cancer['feature_names'])
df.head()

df.info()

df.isnull()

df.describe()

df.corr()

cancer['target']

sns.heatmap(df)

df['cancer']=pd.DataFrame(cancer['target'])
df.head()

sns.countplot(x='cancer',data=df,palette='RdBu_r')

l=list(df.columns[0:10])
for i in range(len(l)-1):
    sns.boxplot(x='cancer',y=l[i], data=df, palette='winter')
    plt.figure()

l=list(df.columns[11:20])
for i in range(len(l)-1):
    sns.boxplot(x='cancer',y=l[i], data=df, palette='winter')
    plt.figure()

l=list(df.columns[21:30])
for i in range(len(l)-1):
    sns.boxplot(x='cancer',y=l[i], data=df, palette='winter')
    plt.figure()

f,(ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(12,6))
ax1.scatter(df['mean area'],df['cancer'])
ax1.set_title("Cancer cases as a function of mean area", fontsize=15)
ax2.scatter(df['mean smoothness'],df['cancer'])
ax2.set_title("Cancer cases as a function of mean smoothness", fontsize=15)

f,(ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(12,6))
ax1.scatter(df['mean texture'],df['cancer'])
ax1.set_title("Cancer cases as a function of mean texture", fontsize=15)
ax2.scatter(df['mean perimeter'],df['cancer'])
ax2.set_title("Cancer cases as a function of mean perimeter", fontsize=15)

df_feat = df.drop('cancer',axis=1) # Define a dataframe with only features
df_feat.head()

df_target = df['cancer'] # Define a dataframe with only target results i.e. cancer detections
df_target.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_feat, df_target, test_size=0.20, random_state=101)

y_train.head()

from sklearn.svm import SVC

model = SVC()

model.fit(X_train,y_train)

predictions = model.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y_test,predictions))

print(classification_report(y_test,predictions))

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
LR

yhat = LR.predict(X_test)
yhat

yhat_prob = LR.predict_proba(X_test)
yhat_prob

print(confusion_matrix(y_test,yhat))

print(classification_report(y_test,yhat))

