# -*- coding: utf-8 -*-
"""Exp6_N029_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qxlm90Mo92WYe2VOdQd4W9147jIaG6xS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import warnings
import math
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv('/content/Admission_Predict.csv')
df.head()

X = df[['GRE Score','TOEFL Score','University Rating','SOP','LOR ','CGPA','Research']]
Y = df['Chance of Admit ']

# Standardize X & Y
Y = np.array((Y - Y.mean())/Y.std())
X = X.apply(lambda rec: (rec - rec.mean())/rec.std(),axis = 0)

import random
def initialize(dim):
  np.random.seed(seed=48)
  b = random.random()
  theta=np.random.rand(dim)
  return b ,theta

b,theta = initialize(7)
print("Bias: ",b,"Weights: ",theta)

def predict_Y(b,theta,X):
  return b + np.dot(X,theta)

b,theta=initialize(7)
Y_hat=predict_Y(b,theta,X)

Y_hat[0:10]

Y.shape

def get_cost(Y,Y_hat):
  Y_resd=Y-Y_hat      # Calculating the residuals from taking difference between actual and predicted values
  return np.sum(np.dot(Y_resd.T,Y_resd))/(2*len(Y-Y_resd))

b,theta=initialize(7)
Y_hat=predict_Y(b,theta,X)
get_cost(Y,Y_hat)

def update_theta(x,y,y_hat,b_0,theta_o,learning_rate):
  #gradient of bias
  db=(np.sum(y_hat-y))/len(y)
  #gradient of weights
  dw=(np.sum(np.dot((y_hat-y),x)))/len(y)
    #update bias
  b_1=b_0-learning_rate*db
#update theta
  theta_1=theta_o-learning_rate*dw
    #return the new bias and beta values
  return b_1,theta_1

b,theta=initialize(7)
print("After initialization Bias: ",b,"theta: ",theta)
Y_hat=predict_Y(b,theta,X)
b,theta=update_theta(X,Y,Y_hat,b,theta,0.001)
print("After first update -Bias: ",b,"theta: ",theta)

def run_gradient_descent(X,
                         Y,
                         alpha=0.001,
                         num_iterations=500):
# Intialize the bias and weights
  b,theta=initialize(X.shape[1])
  iter_num=0
  # gd_iterations_df keeps track of the cost every 10 iterations
  gd_iterations_df=pd.DataFrame(columns=['iteration','cost'])
  result_idx=0
    
    # Run the iterations in loop

  for each_iter in range(num_iterations):
    Y_hat=predict_Y(b,theta,X)          # Calcuated predicted value of y
    this_cost=get_cost(Y,Y_hat)         # Calculate the cost
    prev_b=b                            # Save the previous bias and weights
    prev_theta=theta
    b,theta=update_theta(X,Y,Y_hat,prev_b,prev_theta,alpha)       # Update and calculate the new values of bias and weights

    if(iter_num%10==0):                                       # For every 10 iterations, store the cost i.e. MSE
      gd_iterations_df.loc[result_idx]=[iter_num,this_cost]
      result_idx=result_idx+1
    iter_num +=1
  print("Final Estimate of b and theta : ",b,theta)
#return the final bias, weights and the cost at the end

  return gd_iterations_df,b,theta

gd_iterations_df,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=500)

gd_iterations_df[0:20]

plt.plot(gd_iterations_df['iteration'],gd_iterations_df['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

alpha_df_1,b,theta=run_gradient_descent(X,Y,alpha=0.01,num_iterations=500)
alpha_df_1[0:20]

plt.plot(alpha_df_1['iteration'],alpha_df_1['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

alpha_df_2,b,theta=run_gradient_descent(X,Y,alpha=0.1,num_iterations=500)
alpha_df_2[0:20]

plt.plot(alpha_df_2['iteration'],alpha_df_2['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

plt.plot(gd_iterations_df['iteration'],gd_iterations_df['cost'],label="alpha=0.001")
plt.plot(alpha_df_1['iteration'],alpha_df_1['cost'],label="alpha=0.01")
plt.plot(alpha_df_2['iteration'],alpha_df_2['cost'],label="alpha=0.1")
plt.legend()
plt.ylabel('cost')
plt.xlabel('Number of iterations')
plt.title('Cost Vs. Iterations for different alpha values')

alpha_df_3,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=2000)
print(alpha_df_3[0:20])
plt.subplot(2,2,1)
plt.plot(alpha_df_3['iteration'],alpha_df_3['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

print("-----------------------------------------------------------")

alpha_df_4,b,theta=run_gradient_descent(X,Y,alpha=0.01,num_iterations=2000)
print(alpha_df_4[0:20])
plt.subplot(2,2,2)
plt.plot(alpha_df_4['iteration'],alpha_df_4['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

print("-----------------------------------------------------------")

alpha_df_5,b,theta=run_gradient_descent(X,Y,alpha=0.1,num_iterations=2000)
print(alpha_df_5[0:20])
plt.subplot(2,2,3)
plt.plot(alpha_df_5['iteration'],alpha_df_5['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

alpha_df_5,b,theta=run_gradient_descent(X,Y,alpha=0.1,num_iterations=2000)

plt.plot(alpha_df_3['iteration'],alpha_df_3['cost'],label="alpha=0.001")
plt.plot(alpha_df_4['iteration'],alpha_df_4['cost'],label="alpha=0.01")
plt.plot(alpha_df_5['iteration'],alpha_df_5['cost'],label="alpha=0.1")
plt.legend()
plt.ylabel('cost')
plt.xlabel('Number of iterations')
plt.title('Cost Vs. Iterations for different alpha values')

"""For alpha=0.1 and no.of iterations = 2000 it is overshooting , therefore the values displayed are nan"""

alpha_df_6,b,theta=run_gradient_descent(X,Y,alpha=0.001,num_iterations=5000)
print(alpha_df_6[0:20])
plt.subplot(2,2,1)
plt.plot(alpha_df_6['iteration'],alpha_df_6['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

print("-----------------------------------------------------------")

alpha_df_7,b,theta=run_gradient_descent(X,Y,alpha=0.01,num_iterations=5000)
print(alpha_df_7[0:20])
plt.subplot(2,2,2)
plt.plot(alpha_df_7['iteration'],alpha_df_7['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

print("-----------------------------------------------------------")

alpha_df_8,b,theta=run_gradient_descent(X,Y,alpha=0.1,num_iterations=5000)
print(alpha_df_8[0:20])
plt.subplot(2,2,3)
plt.plot(alpha_df_8['iteration'],alpha_df_8['cost'])
plt.xlabel("Number of iterations")
plt.ylabel("Cost or MSE")

alpha_df_8,b,theta=run_gradient_descent(X,Y,alpha=0.1,num_iterations=5000)

plt.plot(alpha_df_6['iteration'],alpha_df_6['cost'],label="alpha=0.001")
plt.plot(alpha_df_7['iteration'],alpha_df_7['cost'],label="alpha=0.01")
plt.plot(alpha_df_8['iteration'],alpha_df_8['cost'],label="alpha=0.1")
plt.legend()
plt.ylabel('cost')
plt.xlabel('Number of iterations')
plt.title('Cost Vs. Iterations for different alpha values')

"""For alpha=0.1 and no.of iterations = 5000 it is overshooting , therefore the values displayed are nan

**Conclusion:**

For learning rate = 0.1 as the learning rate increases equal to or greater than 2000 the optimal pt is overshooting. 

As the learning rate increases, we see a decrease in the cost function.
"""

